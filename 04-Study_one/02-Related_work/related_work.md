# Related work

Many studies have been reported to investigate specific design features to
support collaborative information analysis. For example, Goyal and Fussell
[@Goyal2016] studied the effect of hypotheses sharing on sensemaking. Mahyar and
Tory [@Mahyar2013] designed a visualization to connect collaborators' common
findings and evaluated its support for team performance. Hajizadeh et al.
[@Hajizadeh2013] explored how sharing teammate's interactions affects awareness.
These studies report interesting results of controlled lab studies to validate
hypotheses of specific design features. However, they do not provide insight on
how teams would collaborate with a complete tool as nexus of features in the
real world over extended period of time.

Field studies were conducted aimed to understand design requirements of
collaborative information analysis in more realistic settings. Chin et al.
[@Chin2009] observed and analyzed the analytic strategies, work practices, tools
ad collaboration norms of professional intelligence analysts. Kang and Stasko
[@Kang2011] studied how student analysts, as in our study, completed in-class
intelligence projects. Carroll et al. [@Carroll2013] attempted to model a
complex analytic task scenario in a lab setting, and examined the development of
team awareness in a four-hour-long task. These studies helped improve
understanding of current work practice with state-of-the-art tools or no tools
at all. However, it remains unknown how teams would behave with technology that
is explicitly designed for collaborative information analysis.

Before the study with our tool, we took an informal observation in class.
Students learned several analytic techniques, including IEW (a technique to
extract and assess values of evidence), ACH (a technique to evaluate multiple
hypotheses against evidence), timeline analysis and network analysis, as well as
state-of-the-art tools to facilitate these techniques. Two weeks before our
study, students practiced applying these techniques in a hands-on project. A
typical workflow started with IEW to extract and model data from documents.
Students then replicated key facts into analytic artifacts such as an ACH Matrix
in PARC ACH, a timeline and a network graph in Analyst's Notebook. They had to
repeat the process for each different tool because the data cannot be shared and
carried over directly. Most tools they used lacked serious collaboration support
(except that some teams used Google Doc to construct an IEW table). Analysts
were unable to contribute simultaneously (also known as production blocking).
The analysis work was often divided by tools: each individual created and
analyzed an artifact with a tool on their own. This had the consequence that
findings and hypotheses be made without integrating collective efforts and
diverse knowledge. Analysts must coordinate work by manually sharing notebooks
or graphs, resulting in a scattered placement of results, requiring repeated
manual resynchronizing to identify redundant or missing pieces of information,
analysis of information, and analytic hypotheses. The instructor and students in
our study were aware of the shortcomings of available tools with respect to
support of collaboration.
