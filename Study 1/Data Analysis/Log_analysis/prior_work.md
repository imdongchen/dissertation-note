# Prior work on log analysis

Guo et al. (2016) have a good argument why log analysis is important and what work has been done

From Guo, H. et al. 2016. A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights. IEEE Transactions on Visualization and Computer Graphics. 22, 1 (2016), 51–60.

> Studying how analysts use interaction in visualization systems is an important part of evaluating how well these interactions support analysis needs, like generating insights or performing tasks. Histories of user interactions have been used to advance our understanding of tool usage and user goals in a variety of areas (e.g. [2, 7, 17]). For visual analytics tools, user interaction histories contain information about the sequence of choices that analysts make when exploring data or per- forming a task. They help evaluators identify which of the available interactions in an application are preferred by analysts or are part of a critical path of interactions that is necessary for a task.

> In the past, both automatic analyses of interactions and manual reviews have resulted in discovering design improvements for visualiza- tion applications and workflows. In some cases, observing interactions is the basis for cognitive and behavioral models of end users, and these models can predict the outcome of design changes to applications and workflows [14], including benchmark tasks for visualization [11]. In other cases, interaction logs have been used to classify users with dif- ferent task strategies and personality traits, as well as to predict the performance of basic visualization tasks like visual search [5].

> In addition to predictive models of behavior, identifying the roles of specific interactions in the analysis process has led to descriptive mod- els of how analysts use visualization to make sense of data, including the learning loop complex [26], and Pirolli and Card’s cascading for- aging and sensemaking loops [23]. In a case study with the popular visual analytics application Jigsaw, Kang et al. found that analysts’ in- teraction histories showed evidence of these high-level sensemaking processes [18]. Similarly, Boyandin et al. [4] visualized user inter- action logs to compare the use of interaction techniques to arrive at findings with flow maps under animation versus small-multiples conditions. Reda et al. [25] approached interaction and sensemaking from a different angle, combining interaction logs and user-reported mental processes into an extended log and modeling the log using transition diagrams to better understand the transition between mental and interaction states. Like Kang’s case study, we identify and interpret patterns of interactions with a visual analytics application using a qualitative review process. In addition, we describe how a quantitative, automated analysis of interaction histories using methods including but not limited to transition diagram analysis led us toward focused questions to answer during the qualitative review. In Section 3, we describe the quantitative and qualitative stages of interaction analysis.

> Another way that interaction histories have been used in visualization applications is to identify states that make navigating or reason- ing about an application easier. Heer et al. proposed a model for analyzing visualization interaction histories and used it to identify us- age patterns for Tableau [15]. In doing so, they developed “hand-crafted”, application-specific chunking rules that group low-level interactions into a manageable sequence of states. We build on this idea by proposing an automatic approach for chunking interactions based on frequently occurring patterns in a collection of end users’ histories. Another kind of abstraction is categorizing individual actions into top-level categories; this step simplifies the process of identifying interaction sequences with similar semantics but different low-level details (e.g., adjusting the range of a data filter using different inter- actions). A few taxonomies have been proposed to characterize user interactions with visual analytics systems from a high level. Some of the taxonomies focuss solely on data analysis tasks, such as Zhou and Feiner [34] and Amar et al. [3]. Heer and Shneiderman [16], on the other hand, proposed a taxonomy that captures three types of tasks in iterative visual analysis: data and view specification, view manipulation, and provenance. Our categories are based on a taxonomy of seven general interaction types that Yi et al. describe based on a review of interactive information visualizations [32]. We chose to build on this taxonomy in coding interactions for our case study because it has coverage over most of the behaviors we observed. We describe our coding process in more detail in Section 4.4.

> Previous visual analytic applications, such as HARVEST [13], have included features to track insight provenance as end users interact. A benefit of doing this capture at the application-level rather than cod- ing interactions post hoc is that displaying information about semantic actions to analysts could help them perform tasks better, as Gotz et al. found [12]. However, the automatic-capture approach requires instrumenting the visual analytic application, so it cannot be used to evaluate deployed applications, as in our case study.

> Recovering longer reasoning processes by observing interactions is difficult. For example, knowing when one reasoning process ends and another begins may be unclear from a sequence of interaction alone. Previously, Dou et al. [8] demonstrated that interaction logs from a visual analytics tool can be visually examined and coded by humans to recover analysts’ reasoning processes, such as specific findings and strategies. Similar to this work, we performed an exploratory user study of a visual analytics application, then used video and a visual- ization of participants’ interactions to recover strategies. Unlike Dou et al.’s study, we first used an automatic analysis of interaction logs in order to focus the qualitative review of participants’ interactions. Based on our experience in the case study, we believe this focusing step leads evaluators toward new hypotheses and is helpful in making manual reviews of large datasets more tractable.

In particular, log analysis has been conducted:
- to model cognition and behavior of end users [11, 14]
- to classify users with different task strategies and personality traits, and to predict performance [5]
- to identify the role of specific interactions in analysis [26, 23, 18, 4, 25]
- to identify states that make navigating or reasoning easier [15]
- action taxonomy [34, 3, 16, 32]
- to recover analyst’s reasoning process [8]

These papers are cited as relevant to log analysis
1. [2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In Proceedings of the 29th an- nual international ACM SIGIR conference on Research and development in information retrieval, pages 19–26. ACM, 2006
1. [3] R. Amar, J. Eagan, and J. Stasko. Low-level components of analytic activity in information visualization. In Information Visualization, 2005. INFOVIS 2005. IEEE Symposium on, pages 111–117. IEEE, 2005.
1. [4] I. Boyandin, E. Bertini, and D. Lalanne. A qualitative study on the ex- ploration of temporal changes in flow maps with animation and small- multiples. In Computer Graphics Forum, volume 31, pages 1005–1014. Wiley Online Library, 2012.
1. [5] E. T. Brown, A. Ottley, H. Zhao, Q. Lin, R. Souvenir, A. Endert, and R. Chang. Finding waldo: Learning about users from their interac- tions. Visualization and Computer Graphics, IEEE Transactions on, 20(12):1663–1672,
1. [7] E. H. Chi, P. Pirolli, K. Chen, and J. Pitkow. Using information scent to model user information needs and actions and the web. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 490–497. ACM, 2001.
1. [8] W. Dou, D. H. Jeong, F. Stukes, W. Ribarsky, H. R. Lipford, and R. Chang. Recovering reasoning processes from user interactions. IEEE Computer Graphics and Applications, (3):52–61, 2009.
1. [11] S. Gomez and D. Laidlaw. Modeling task performance for a crowd of users from interaction histories. In Proceedings of the SIGCHI Confer- ence on Human Factors in Computing Systems, pages 2465–2468. ACM, 2012
1. [14] W. D. Gray, B. E. John, and M. E. Atwood. Project ernestine: Validating a goms analysis for predicting and explaining real-world task performance. Human-computer interaction, 8(3):237–309, 1993.
1. [15] J. Heer, J. Mackinlay, C. Stolte, and M. Agrawala. Graphical histories for visualization: Supporting analysis, communication, and evaluation. Vi- sualization and Computer Graphics, IEEE Transactions on, 14(6):1189– 1196, 2008.
1. [16] J. Heer and B. Shneiderman. Interactive dynamics for visual analysis. Queue, 10(2):30, 2012.
1. [17] E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and K. Rommelse. The lumiere project: Bayesian user modeling for inferring the goals and needs of software users. In Proceedings of the Fourteenth conference on Uncer- tainty in artificial intelligence, pages 256–265. Morgan Kaufmann Pub- lishers Inc., 1998.
1. [18] Y.-a. Kang, C. Gorg, and J. Stasko. Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study. In Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Sym- posium on, pages 139–146. IEEE, 2009.
1. [25] K. Reda, A. E. Johnson, J. Leigh, and M. E. Papka. Evaluating user behavior and strategy during visual exploration. In Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization, pages 41–45. ACM, 2014.
1. [26] D. M. Russell, M. J. Stefik, P. Pirolli, and S. K. Card. The cost structure of sensemaking. In Proceedings of the INTERACT’93 and CHI’93 con- ference on Human factors in computing systems, pages 269–276. ACM, 1993.
1. [32] J. S. Yi, Y. ah Kang, J. T. Stasko, and J. A. Jacko. Toward a deeper under- standing of the role of interaction in information visualization. Visualiza- tion and Computer Graphics, IEEE Transactions on, 13(6):1224–1231, 2007.
1. [33] J. S. Yi, Y.-a. Kang, J. T. Stasko, and J. A. Jacko. Understanding and characterizing insights: how do people gain insights using information visualization? In Proceedings of the 2008Workshop on BEyond time and errors: novel evaLuation methods for Information Visualization, page 4. ACM, 2008.
1. [34] M. X. Zhou and S. K. Feiner. Visual task characterization for auto- mated visual discourse synthesis. In Proceedings of the SIGCHI con- ference on Human factors in computing systems, pages 392–399. ACM Press/Addison-Wesley Publishing Co., 1998.
