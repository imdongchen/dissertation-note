# Discussion

## Reflections on method

Though these earlier lab studies produced interesting results, one of the biggest limiting factors was the constraint of time, and thus the simplified task. Most lab studies finish within an hour (some exceptions may last up to a few hours). To allow teams to be able to solve the task as well as coordinate team processes, the task content and complexity were significantly reduced. For example, in [@Convertino2012], participants only worked on 25 facts, and the task was essentially a counting problem--the option with the least cons was the answer.

Besides, the limited time may prevent teams from developing sufficient awareness to work properly. Lab studies look at a completely new setting: a new formed team, a new collaborative tool, a new task, and a new environment. It takes time for teams to establish common ground, and to appropriate the tool to best serve their purpose. Under time pressure teams would either ignore team communication completely or gain not enough awareness to collaborate smoothly. The study thus may not be able to produce results that reflect how teams would really use a tool.

Kang and Stasko [@Kang2011] observed that intelligence analysis is a process of framing and re-framing problems. As analysts collected more information, their framing of the problem started to change and motivated them to reconsider the problem. This process takes time. When under such a limited time constraint as in a lab study, participants strive to finish the task with no extra cognition power to reconsider the problem.

Another factor that influences teamwork is group size. Many lab studies recruit two participants as a team. Interaction in a two-person group is the most simplified form of group interaction because A has to interact with B and B has to respond to A. Even a three-person group would produce more complex interaction patterns that a two-person group would normally not. For example,
social loafing,
cognitive specialization

## Visualization could cases bias
Visualization can cause bias [@Mussweiler2000], for example, one team connected all theft cases to a single suspect. While this was only an inference, the network graph exerted a strong bias toward the expression that the thefts were all connected and were committed by a single group. The team eventually agreed on this wrong conclusion and ignored all other evidence showing conflicts between cases.

Mussweiler T, Strack F (2000) Numeric judgments under uncertainty: The role of knowledge in anchoring. Journal of Experimental Social Psychology 36:495-518

## Interweaving of information collection and analysis
Re-representation, reframing of the problem, restructuring of information foraging

## Instrument for teaching information analytics
Anecdotal reflections from the instructor suggested that the system can include support for instructor intervention. During the study, the instructor would go over to students and checked their computer screen on how they were doing. He was interested in students' analytic status, such as the entities students had created and hypotheses they had developed. CAnalytics could provide such facilities because students interaction logs are already captured, but only streamed to team members for awareness. Instructors may also benefit from the streaming and visualization of student logs to decide when to intervene. And intervening in real time is likely to be more efficient than doing post study.

According to Heuer's claim [@Heuer1999]: 

> Analysts should be self-conscious about their reasoning process. They should think about how they make judgements and reach conclusions, not just about the judgments and conclusions themselves.

Thus more attention must be paid on the process of analysis, in addition to the result itself. Assessment on student performance should also take into account what students did before they composed the final analytic report. The system logs provide instructors an opportunity to learn more about student's problem-solving process.

Other possible related papers: @Burns2002

## Technology appropriation
Based on students' feedback, two teams decided to change the appropriation of the tool halfway in their analysis. For instance, one team started with dividing work by case documents, but later decided members should annotate different entity types. Another team started off by annotating all entities, whether related to unrelated to their problem. Later they discovered that this strategy brought too much noise instead of useful information, and wanted to start over. This phenomenon that teams learn to use a tool and adapt to their teamwork is common in many previously reported work. The phenomenon occurs when the team together with the tool works better like a system given a sufficiently long period of time. This is also where a field study has advantage over a lab study, as a lab study is often too short to observe a complete team development.

## View sharing vs control
One major critic was the lack of view sharing support in the tool. Students reflected that their team had difficulty coordinating their findings because they could not see collaborator's view directly. While view sharing has been identified as an important awareness contributor, the design of view sharing becomes a more complex issue in the multiple coordinated view collaboration.

- private vs public view
- shared workspace (collaborative editor)
- handoff
